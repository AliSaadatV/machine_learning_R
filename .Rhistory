install.packages("AmesHousing")
ames <- AmesHousing::make_ames()
ames
AmesHousing::make_ames()
AmesHousing::make_ames
View(ames)
AmesHousing::make_ames()
AmesHousing::ames_raw
AmesHousing::make_ordinal_ames()
AmesHousing::make_ordinal_ames
AmesHousing::make_ames()
AmesHousing::make_ames
AmesHousing::make_ames_new()
AmesHousing::make_ames_new
AmesHousing::ames_new
AmesHousing::ames_new()
AmesHousing::ames_new
install.packages("rsample")
install.packages("rsample")
install.packages("rsample")
install.packages("rsample")
install.packages("rsample")
rsample::attrition
data("attrition", package = "modeldata")
install.packages("modeldata")
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
# Ames housing data
ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)
ames.h2o
ames.h2o[1]
ames.h2o
View(ames)
# Job attrition data
churn <- rsample::attrition %>%
mutate_if(is.ordered, .funs = factor, ordered = FALSE)
churn <- modeldata::attrition
View(churn)
churn.h2o <- as.h2o(churn)
# Job attrition data
churn <- modeldata::attrition %>%
mutate_if(is.ordered, .funs = factor, ordered = FALSE)
churn.h2o <- as.h2o(churn)
View(churn.h2o)
churn.h2o
round(nrow(ames) * 0.7)
### Simple sampling
# Using base R
set.seed(123)  # for reproducibility
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]
index_2 <- createDataPartition(ames$Sale_Price, p = 0.7,
list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]
View(index_2)
indee
head(index_2)
split_1  <- initial_split(ames, prop = 0.7)
split_1
train_3  <- training(split_1)
test_3   <- testing(split_1)
# Using h2o package
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7,
seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
test_4
### Stratified sampling
# orginal response distribution
table(churn$Attrition) %>% prop.table()
# stratified sampling with the rsample package
set.seed(123)
split_strat  <- initial_split(churn, prop = 0.7,
strata = "Attrition")
train_strat  <- training(split_strat)
test_strat   <- testing(split_strat)
# consistent response ratio between train & test
table(train_strat$Attrition) %>% prop.table()
##
##       No      Yes
## 0.838835 0.161165
table(test_strat$Attrition) %>% prop.table()
?caret::trainControl()
model_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) +
ns(Latitude, df = 3), data = ames)
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)
model_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) +
ns(Latitude, df = 3), data = ames)
model_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) + ns(Latitude, df = 3), data = ames)
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
### All together
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames, prop = 0.7,
strata = "Sale_Price")
# Ames housing data
ames <- AmesHousing::make_ames()
split <- initial_split(ames, prop = 0.7,
strata = "Sale_Price")
split
split$data
split$in_id
split$id
split$out_id
split
ames_train  <- training(split)
ames_test   <- testing(split)
# Specify resampling strategy
cv <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 5
)
cv
# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal())  %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_pca(all_numeric(), -all_outcomes())
library(recipes)  # for feature engineering tasks
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal())  %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_pca(all_numeric(), -all_outcomes())
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
# Stratified sampling with the rsample package
set.seed(123)
ames <- AmesHousing::make_ames()
split <- initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
# Specify resampling plan
cv <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 5
)
# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
# Tune a knn model using grid search
knn_fit2 <- train(
blueprint,
data = ames_train,
method = "knn",
trControl = cv,
tuneGrid = hyper_grid,
metric = "RMSE"
)
ames <- AmesHousing::make_ames()
split <- rsample::initial_split(ames, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
ames_test <- rsample::testing(split)
set.seed(123)
cv_model2 <- train(
Sale_Price ~ Gr_Liv_Area + Year_Built,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
# Modeling packages
library(caret)    # for cross-validation, etc.
cv_model2 <- train(
Sale_Price ~ Gr_Liv_Area + Year_Built,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
cv_model2
resamples(cv_model2)
resamples(list(cv_model2))
summary(cv_model2)
set.seed(123)
cv_model3 <- train(
Sale_Price ~ .,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 5)
)
summary(cv_model3)
str(ames_train)
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
# Helper packages
library(dplyr)    # for data manipulation
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
library(recipes)
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
cv <- trainControl(method = "cv", number = 5)
lm_model3 <- train(blueprint,
data = ames_train,
method = "lm",
trControl = cv)
cv_model2$finalModel
cv_model3$finalModel
df1 <- broom::augment(cv_model2$finalModel, data = ames_train)
View(df1)
?train
churn <- modeldata::attrition
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
library(rsample)   # for data splitting
# Modeling packages
library(caret)     # for logistic regression modeling
# Model interpretability packages
library(vip)       # variable importance
glimpse(churn)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
View(churn_train)
churn <- modeldata::attrition
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
View(churn_test)
?modeldata::attrition
churn$OverTime
View(churn)
### logistic regression
set.seed(123)
cv_model <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
# predict class
pred_class <- predict(cv_model, churn_train)
pred_class
# create confusion matrix
confusionMatrix(
data = relevel(pred_class, ref = "Yes"),
reference = relevel(churn_train$Attrition, ref = "Yes")
)
table(churn_train$Attrition) %>% prop.table()
### logistic regression
set.seed(123)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
Attrition ~ MonthlyIncome + OverTime,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model3 <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
# extract out of sample performance measures
summary(
resamples(
list(
model1 = cv_model1,
model2 = cv_model2,
model3 = cv_model3
)
)
)$statistics$Accuracy
### Confusion matrix
# predict class
pred_class <- predict(cv_model3, churn_train)
# create confusion matrix
confusionMatrix(
data = relevel(pred_class, ref = "Yes"),
reference = relevel(churn_train$Attrition, ref = "Yes")
)
pred_class
predict(cv_model1, churn_train, type = "prob")
### ROC analysis
library(ROCR)
# Compute predicted probabilities
m1_prob <- predict(cv_model1, churn_train, type = "prob")$Yes
m2_prob <- predict(cv_model2, churn_train, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_train, type = "prob")$Yes
?predict
?caret::predict
?prediction
prediction(m1_prob, churn_train$Attrition)
prediction(m1_prob, churn_train$Attrition)->q
q
q@predictions
performance(q)
performance(q, measure = "tpr")
# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m2_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf3 <- prediction(m3_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf1
perf1@x.name
perf1@y.name
perf1@x.values
rm(q)
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
plot(perf3, add = TRUE, col = "gray")
legend(0.8, 0.2, legend = c("cv_model1", "cv_model2", "cv_model3"),
col = c("black", "blue", "gray"), lty = 2:1, cex = 0.6)
### Partial least squares (reduces number of numeric features, then performs logistic regression)
# Perform 10-fold CV on a PLS model tuning the number of PCs to
# use as predictors
set.seed(123)
cv_model_pls <- train(
Attrition ~ .,
data = churn_train,
method = "pls",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 16
)
# Model with lowest RMSE
cv_model_pls$bestTune
cv_model_pls
# results for model with lowest loss
cv_model_pls$results %>%
dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
# Plot cross-validated RMSE
ggplot(cv_model_pls)
### Interpretation
vip(cv_model3, num_features = 20)
churn <- modeldata::attrition %>%  mutate_if(is.ordered, .funs = factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
set.seed(123)
cv_model3 <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
### Partial least squares (reduces number of numeric features, then performs logistic regression)
# Perform 10-fold CV on a PLS model tuning the number of PCs to
# use as predictors
set.seed(123)
cv_model_pls <- train(
Attrition ~ .,
data = churn_train,
method = "pls",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 16
)
# Model with lowest RMSE
cv_model_pls$bestTune
# Plot cross-validated RMSE
ggplot(cv_model_pls)
### Interpretation
vip(cv_model3, num_features = 20)
### logistic regression
set.seed(123)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
Attrition ~ MonthlyIncome + OverTime,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
### Confusion matrix
# predict class
pred_class <- predict(cv_model3, churn_train)
