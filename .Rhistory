library(vip)       # variable importance
glimpse(churn)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
View(churn_train)
churn <- modeldata::attrition
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
View(churn_test)
?modeldata::attrition
churn$OverTime
View(churn)
### logistic regression
set.seed(123)
cv_model <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
# predict class
pred_class <- predict(cv_model, churn_train)
pred_class
# create confusion matrix
confusionMatrix(
data = relevel(pred_class, ref = "Yes"),
reference = relevel(churn_train$Attrition, ref = "Yes")
)
table(churn_train$Attrition) %>% prop.table()
### logistic regression
set.seed(123)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
Attrition ~ MonthlyIncome + OverTime,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model3 <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
# extract out of sample performance measures
summary(
resamples(
list(
model1 = cv_model1,
model2 = cv_model2,
model3 = cv_model3
)
)
)$statistics$Accuracy
### Confusion matrix
# predict class
pred_class <- predict(cv_model3, churn_train)
# create confusion matrix
confusionMatrix(
data = relevel(pred_class, ref = "Yes"),
reference = relevel(churn_train$Attrition, ref = "Yes")
)
pred_class
predict(cv_model1, churn_train, type = "prob")
### ROC analysis
library(ROCR)
# Compute predicted probabilities
m1_prob <- predict(cv_model1, churn_train, type = "prob")$Yes
m2_prob <- predict(cv_model2, churn_train, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_train, type = "prob")$Yes
?predict
?caret::predict
?prediction
prediction(m1_prob, churn_train$Attrition)
prediction(m1_prob, churn_train$Attrition)->q
q
q@predictions
performance(q)
performance(q, measure = "tpr")
# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m2_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf3 <- prediction(m3_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf1
perf1@x.name
perf1@y.name
perf1@x.values
rm(q)
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
plot(perf3, add = TRUE, col = "gray")
legend(0.8, 0.2, legend = c("cv_model1", "cv_model2", "cv_model3"),
col = c("black", "blue", "gray"), lty = 2:1, cex = 0.6)
### Partial least squares (reduces number of numeric features, then performs logistic regression)
# Perform 10-fold CV on a PLS model tuning the number of PCs to
# use as predictors
set.seed(123)
cv_model_pls <- train(
Attrition ~ .,
data = churn_train,
method = "pls",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 16
)
# Model with lowest RMSE
cv_model_pls$bestTune
cv_model_pls
# results for model with lowest loss
cv_model_pls$results %>%
dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
# Plot cross-validated RMSE
ggplot(cv_model_pls)
### Interpretation
vip(cv_model3, num_features = 20)
churn <- modeldata::attrition %>%  mutate_if(is.ordered, .funs = factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(churn, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
set.seed(123)
cv_model3 <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
### Partial least squares (reduces number of numeric features, then performs logistic regression)
# Perform 10-fold CV on a PLS model tuning the number of PCs to
# use as predictors
set.seed(123)
cv_model_pls <- train(
Attrition ~ .,
data = churn_train,
method = "pls",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 16
)
# Model with lowest RMSE
cv_model_pls$bestTune
# Plot cross-validated RMSE
ggplot(cv_model_pls)
### Interpretation
vip(cv_model3, num_features = 20)
### logistic regression
set.seed(123)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
Attrition ~ MonthlyIncome + OverTime,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
### Confusion matrix
# predict class
pred_class <- predict(cv_model3, churn_train)
ames <- AmesHousing::make_ames()
# Helper packages
library(recipes)  # for feature engineering
# Modeling packages
library(glmnet)   # for implementing regularized regression
library(caret)    # for automating the tuning process
# Model interpretability packages
library(vip)
set.seed(123)
split <- rsample::initial_split(anes, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
split <- rsample::initial_split(anes, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
split <- rsample::initial_split(ames, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
ames_test <- rsample::testing(split)
# Create training  feature matrices
# we use model.matrix(...)[, -1] to discard the intercept
X <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
dim(x)
dim(X)
head(X)
?model.matrix
# transform y with log transformation
Y <- log(ames_train$Sale_Price)
X
### Ridge
# Apply ridge regression to ames data
ridge <- glmnet(
x = X,
y = Y,
alpha = 0
)
plot(ridge, xvar = "lambda")
ridge
ridge$lambda
names(X)
col_names(X)
colnames(X)
### Using caret package to find the best lambda (how much penalty) and alpha (alpha=0 --> Ridge, alpha=1 --> Lasso, betweeon 0 and 1 --> ENET)
# grid search across
cv_glmnet <- train(
x = X,
y = Y,
method = "glmnet",
preProc = c("zv", "center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# model with lowest RMSE
cv_glmnet$bestTune
# results for model with lowest RMSE
cv_glmnet$results %>%
filter(alpha == cv_glmnet$bestTune$alpha, lambda == cv_glmnet$bestTune$lambda)
# plot cross-validated RMSE
ggplot(cv_glmnet)
ames <- AmesHousing::make_ames()
set.seed(123)
split <- rsample::initial_split(ames, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
ames_test <- rsample::testing(split)
### Fit a basic MARS model
mars1 <- earth(
Sale_Price ~ .,
data = ames_train
)
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
# Modeling packages
library(earth)     # for fitting MARS models
library(caret)     # for automating the tuning process
# Model interpretability packages
library(vip)       # for variable importance
library(pdp)       # for variable relationships
### Fit a basic MARS model
mars1 <- earth(
Sale_Price ~ .,
data = ames_train
)
,ars1
mars1
mars1$selected.terms
mars1$cuts
mars1
summary(mars1)
coef(mars1)
coef(mars1) %>% head(10)
ames$Functional
plot(mars1, which = 1)
seq(2, 100, length.out = 10)
### Tunning
# create a tuning grid
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
head(hyper_grid)
hyper_grid
hyper_grid$degree
hyper_grid$nprune
hyper_grid[1,1]
hyper_grid[1,]
hyper_grid[2,]
hyper_grid[3,]
# Helper packages
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics
library(rsample)    # for creating validation splits
library(recipes)    # for feature engineering
# create training (70%) set for the rsample::attrition data.
attrit <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(attrit, prop = .7, strata = "Attrition")
# create training (70%) set for the rsample::attrition data.
attrit <- modeldata::attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(attrit, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes())
?bake
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
prep()
bake(blueprint)
bake(blueprint, new_data = head(churn_train))
q <- bake(blueprint, new_data = head(churn_train))
View(q)
glimpse(churn_train)
?prep
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal())
q <- bake( prep(blueprint), new_data = head(churn_train))
glimpse(q)
?all_nominal
summary(blueprint)
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction"))
q <- bake( prep(blueprint), new_data = head(churn_train))
View(q)
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement)
q <- bake( prep(blueprint), new_data = head(churn_train))
View(q)
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
q <- bake( prep(blueprint), new_data = head(churn_train))
View(q)
glimpse(churn_train)
glimpse(q)
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes())
q <- bake( prep(blueprint), new_data = head(churn_train))
glimpse(q)
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
step_center(all_numeric(), -all_outcomes()) %>%
step_scale(all_numeric(), -all_outcomes()) %>%
step_nzv(all_nominal()) %>%
step_integer(contains("Satisfaction")) %>%
step_integer(WorkLifeBalance) %>%
step_integer(JobInvolvement) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
qq <- bake( prep(blueprint), new_data = head(churn_train))
glimpse(qq)
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
# Modeling packages
library(rpart)       # direct engine for decision tree application
library(caret)       # meta engine for decision tree application
# Model interpretability packages
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance
library(pdp)         # for feature effects
ames <- AmesHousing::make_ames()
set.seed(123)
split <- rsample::initial_split(ames, prop=0.7, strata="Sale_Price")
ames_train <- rsample::training(split)
ames_test <- rsample::testing(split)
table(ames_train$Overall_Qual)
install.packages("pRRophetic")
BiocManager::install("pRRophetic")
parallel::detectCores()
DBI::dbConnect(RSQLite::SQLite(), "~/Desktop/dcdb.sqlite")
mydb <- DBI::dbConnect(RSQLite::SQLite(), "~/Desktop/dcdb.sqlite")
DBI::dbConnect(mydb)
library(RSQLite)
db <- dbConnect(dbDriver("SQLite"),
dbname = "~/Desktop/dcdb.sqlite")
dbListTables(db)
rm(mydb)
aact_combs <- dbReadTable(db,"aact_combs")
View(aact_combs)
aact_combs_with_identifiers <- dbReadTable(db,"aact_combs_with_identifiers")
View(aact_combs_with_identifiers)
conditions <- dbReadTable(db,"conditions")
View(conditions)
table(conditions$condition)
library(dplyr)
library(tidyverse)
q <- conditions %>% filter(str_detect(condition, "nervous"))
q <- conditions %>% filter(str_detect(condition, "Nervous"))
View(q)
q <- conditions %>% filter(str_detect(condition_downcase, "multiple"))
q <- conditions %>% filter(str_detect(condition_downcase, "multiple sc"))
qq <- aact_combs_with_identifiers %>% filter(nct_id %in% q$nct_id)
View(qq)
dbListTables(db)
design_group <- dbReadTable(db,"design_group")
View(design_group)
View(q)
MS <- q
View(qq)
MS_ids <- qq
rm(qq.q)
rm(qq,q)
MS_design <- design_group %>% filter(nct_id %in% MS$nct_id)
View(MS_design)
orangebook_combs <- dbReadTable(db,"orangebook_combs")
View(orangebook_combs)
View(MS_design)
all_combs_unormalized <- dbReadTable(db,"all_combs_unormalized")
View(all_combs_unormalized)
MS_combs <- all_combs_unormalized %>% filter(source_id %in% MS$nct_id)
View(MS_combs)
library(tidyverse)
q <- readxl::read_xlsx("~/Desktop/pnas.1803294115.sd02.xlsx")
View(q)
q <- readxl::read_xlsx("~/Desktop/pnas.1803294115.sd02.xlsx", skip = 2)
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "therapeutic efficacy") & str_detect(`Sentences describing the reported drug-drug interactions`, "can be increased"))
View(qq)
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "therapeutic efficacy"))
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "^The therapeutic efficacy"))
p <- read_delim("~/Downloads/DeepDDI-results_entire-drug-pairs.txt")
head(p)
colnames(p)
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "The theraputic "))
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "therapeutic efficacy"))
View(pp)
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "therapeutic efficacy") & str_detect(`DeepDDI output sentence for the corresponding DDI type`, "increased") )
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "therapeutic efficacy") & str_detect(`DeepDDI output sentence for the corresponding DDI type`, "decreased") )
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "The therapeutic efficacy") & str_detect(`DeepDDI output sentence for the corresponding DDI type`, "can be increased") )
pp <- p %>% filter(`DDI type`==8)
pp <- p %>% filter(`DDI type`==9)
pp <- p %>% filter(`DDI type`==7)
pp <- p %>% filter(str_detect(`DeepDDI output sentence for the corresponding DDI type`, "therapeutic efficacy") & str_detect(`DeepDDI output sentence for the corresponding DDI type`, "decreased") )
pp <- p %>% filter(`DDI type`==70)
q <- readxl::read_xlsx("~/Desktop/pnas.1803294115.sd02.xlsx")
View(q)
q <- readxl::read_xlsx("~/Desktop/pnas.1803294115.sd02.xlsx", skip = 2)
library(tidyverse)
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The theraputic efficacy"))
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The theraputic"))
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The therapeutic efficacy"))
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The therapeutic efficacy") & str_detect(`Sentences describing the reported drug-drug interactions`, "increased"))
qq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The therapeutic efficacy") & str_detect(`Sentences describing the reported drug-drug interactions`, "can be increased"))
qqq <- q %>% filter(str_detect(`Sentences describing the reported drug-drug interactions`, "The therapeutic efficacy") & str_detect(`Sentences describing the reported drug-drug interactions`, "can be decreased"))
install.packages("synapserutils", repos=c("http://ran.synapse.org", "http://cran.fhcrc.org"))
install.packages("synapser", repos = c("http://ran.synapse.org", "http://cran.fhcrc.org"))
install.packages("synapserutils", repos=c("http://ran.synapse.org", "http://cran.fhcrc.org"))
library(synapser)
remove.packages("synapser")
1 + 1
version
library(tidyverse)
q_3202 <- read_delim("~/Desktop/20130606_g1k_3202_samples_ped_population.txt")
View(q_3202)
q_mine <- read_delim("~/Desktop/1kg_sample_names.txt")
View(q_mine)
q_mine <- read_delim("~/Desktop/1kg_sample_names.txt")
colnames(q_mine)
str_replace_all(colnames(q_mine), " ", "")
str_replace_all(colnames(q_mine), " ", "")->q_mine
table(q_3202)
table(q_3202$Superpopulation)
table(q_3202$Population)
179+91+107+157
q_NFE <- q_3202 %>% filter(Superpopulation=="EUR" & Population!="FIN")
q_NFE <- q_3202 %>% filter(Superpopulation=="EUR" & Population!="FIN") %>% filter(SampleID %in% q_mine)
q_NFE
q_NFE <- q_3202 %>% filter(Superpopulation=="EUR" & Population!="FIN") %>% distinct(SampleID) %>% nrow()
q_NFE <- q_3202 %>% filter(Superpopulation=="EUR" & Population!="FIN") %>% filter(SampleID %in% q_mine)
View(q_NFE)
write_delim(as_tibble(q_NFE$SampleID), "kg_404_NFE_sample_ids.txt", delim = "\n")
write_delim(as_tibble(q_NFE$SampleID), "~/Desktop/kg_404_NFE_sample_ids.txt", delim = "\n")
View(q_NFE)
table(q_NFE$Superpopulation)
table(q_NFE$Population)
